{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Google stock price prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdG205KHunw9"
      },
      "outputs": [],
      "source": [
        "#Importing the libraries  \n",
        "import numpy as np  \n",
        "import matplotlib.pyplot as plt  \n",
        "import pandas as pd  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = pd.read_csv('/content/Google_Stock_Price_Train.csv')  \n",
        "training_set = dataset_train.iloc[:, 1:2].values  \n"
      ],
      "metadata": {
        "id": "SrGeRNWGxyYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FCp10Qgm0OrF",
        "outputId": "4b2b3f78-0b7e-4f57-eb0b-2bfa31eae710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Date    Open    High     Low   Close      Volume\n",
              "0  1/3/2012  325.25  332.83  324.97  663.59   7,380,500\n",
              "1  1/4/2012  331.27  333.87  329.08  666.45   5,749,400\n",
              "2  1/5/2012  329.83  330.75  326.89  657.21   6,590,300\n",
              "3  1/6/2012  328.34  328.77  323.68  648.24   5,405,900\n",
              "4  1/9/2012  322.04  322.29  309.46  620.76  11,688,800"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e1ca1b25-cbb7-4e74-8df3-728ef0c60d2e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1/3/2012</td>\n",
              "      <td>325.25</td>\n",
              "      <td>332.83</td>\n",
              "      <td>324.97</td>\n",
              "      <td>663.59</td>\n",
              "      <td>7,380,500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1/4/2012</td>\n",
              "      <td>331.27</td>\n",
              "      <td>333.87</td>\n",
              "      <td>329.08</td>\n",
              "      <td>666.45</td>\n",
              "      <td>5,749,400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1/5/2012</td>\n",
              "      <td>329.83</td>\n",
              "      <td>330.75</td>\n",
              "      <td>326.89</td>\n",
              "      <td>657.21</td>\n",
              "      <td>6,590,300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1/6/2012</td>\n",
              "      <td>328.34</td>\n",
              "      <td>328.77</td>\n",
              "      <td>323.68</td>\n",
              "      <td>648.24</td>\n",
              "      <td>5,405,900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1/9/2012</td>\n",
              "      <td>322.04</td>\n",
              "      <td>322.29</td>\n",
              "      <td>309.46</td>\n",
              "      <td>620.76</td>\n",
              "      <td>11,688,800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1ca1b25-cbb7-4e74-8df3-728ef0c60d2e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e1ca1b25-cbb7-4e74-8df3-728ef0c60d2e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e1ca1b25-cbb7-4e74-8df3-728ef0c60d2e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Preprocessing"
      ],
      "metadata": {
        "id": "JmwHAzjcTLoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc=MinMaxScaler(feature_range=(0,1))\n",
        "training_set_norm=sc.fit_transform(training_set)\n",
        "len(training_set_norm)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzWBcqQE0M0T",
        "outputId": "c07c9fa1-7d8a-4a40-b9a1-c737d0f70f2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1258"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=[]\n",
        "y_train=[]\n",
        "for i in range(60,1258):\n",
        "  X_train.append(training_set_norm[i-60:i,0])\n",
        "  y_train.append(training_set_norm[i])\n",
        "X_train, y_train = np.array(X_train), np.array(y_train) #Because in keras the processings takes place through numpys only"
      ],
      "metadata": {
        "id": "fH7smW1b1Y2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok40WU3LQLgu",
        "outputId": "66d5b058-c52f-4e3c-de23-c34f6d626cc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1198, 60)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reshaping\n",
        "X_train=np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))\n",
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1thdQRPbQ3kb",
        "outputId": "a63e8131-2fb6-4718-8735-7912af46416c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.08581368],\n",
              "        [0.09701243],\n",
              "        [0.09433366],\n",
              "        ...,\n",
              "        [0.07846566],\n",
              "        [0.08034452],\n",
              "        [0.08497656]],\n",
              "\n",
              "       [[0.09701243],\n",
              "        [0.09433366],\n",
              "        [0.09156187],\n",
              "        ...,\n",
              "        [0.08034452],\n",
              "        [0.08497656],\n",
              "        [0.08627874]],\n",
              "\n",
              "       [[0.09433366],\n",
              "        [0.09156187],\n",
              "        [0.07984225],\n",
              "        ...,\n",
              "        [0.08497656],\n",
              "        [0.08627874],\n",
              "        [0.08471612]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.92106928],\n",
              "        [0.92438053],\n",
              "        [0.93048218],\n",
              "        ...,\n",
              "        [0.95475854],\n",
              "        [0.95204256],\n",
              "        [0.95163331]],\n",
              "\n",
              "       [[0.92438053],\n",
              "        [0.93048218],\n",
              "        [0.9299055 ],\n",
              "        ...,\n",
              "        [0.95204256],\n",
              "        [0.95163331],\n",
              "        [0.95725128]],\n",
              "\n",
              "       [[0.93048218],\n",
              "        [0.9299055 ],\n",
              "        [0.93113327],\n",
              "        ...,\n",
              "        [0.95163331],\n",
              "        [0.95725128],\n",
              "        [0.93796041]]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building the RNN\n"
      ],
      "metadata": {
        "id": "0A8Iagm4VC8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the Keras libraries and packages  \n",
        "from keras.models import Sequential  \n",
        "from keras.layers import Dense  \n",
        "from keras.layers import LSTM  \n",
        "from keras.layers import Dropout  "
      ],
      "metadata": {
        "id": "JVKR0C6NVHPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialising the RNN  \n",
        "regressor = Sequential()\n",
        "help(LSTM)"
      ],
      "metadata": {
        "id": "OihftzCyVwAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b941ba5a-6677-49cc-cb95-84cd412a0019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class LSTM in module keras.layers.recurrent_v2:\n",
            "\n",
            "class LSTM(keras.layers.recurrent.DropoutRNNCellMixin, keras.layers.recurrent.LSTM, keras.engine.base_layer.BaseRandomLayer)\n",
            " |  LSTM(*args, **kwargs)\n",
            " |  \n",
            " |  Long Short-Term Memory layer - Hochreiter 1997.\n",
            " |  \n",
            " |  See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n",
            " |  for details about the usage of RNN API.\n",
            " |  \n",
            " |  Based on available runtime hardware and constraints, this layer\n",
            " |  will choose different implementations (cuDNN-based or pure-TensorFlow)\n",
            " |  to maximize the performance. If a GPU is available and all\n",
            " |  the arguments to the layer meet the requirement of the cuDNN kernel\n",
            " |  (see below for details), the layer will use a fast cuDNN implementation.\n",
            " |  \n",
            " |  The requirements to use the cuDNN implementation are:\n",
            " |  \n",
            " |  1. `activation` == `tanh`\n",
            " |  2. `recurrent_activation` == `sigmoid`\n",
            " |  3. `recurrent_dropout` == 0\n",
            " |  4. `unroll` is `False`\n",
            " |  5. `use_bias` is `True`\n",
            " |  6. Inputs, if use masking, are strictly right-padded.\n",
            " |  7. Eager execution is enabled in the outermost context.\n",
            " |  \n",
            " |  For example:\n",
            " |  \n",
            " |  >>> inputs = tf.random.normal([32, 10, 8])\n",
            " |  >>> lstm = tf.keras.layers.LSTM(4)\n",
            " |  >>> output = lstm(inputs)\n",
            " |  >>> print(output.shape)\n",
            " |  (32, 4)\n",
            " |  >>> lstm = tf.keras.layers.LSTM(4, return_sequences=True, return_state=True)\n",
            " |  >>> whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)\n",
            " |  >>> print(whole_seq_output.shape)\n",
            " |  (32, 10, 4)\n",
            " |  >>> print(final_memory_state.shape)\n",
            " |  (32, 4)\n",
            " |  >>> print(final_carry_state.shape)\n",
            " |  (32, 4)\n",
            " |  \n",
            " |  Args:\n",
            " |    units: Positive integer, dimensionality of the output space.\n",
            " |    activation: Activation function to use.\n",
            " |      Default: hyperbolic tangent (`tanh`). If you pass `None`, no activation\n",
            " |      is applied (ie. \"linear\" activation: `a(x) = x`).\n",
            " |    recurrent_activation: Activation function to use for the recurrent step.\n",
            " |      Default: sigmoid (`sigmoid`). If you pass `None`, no activation is\n",
            " |      applied (ie. \"linear\" activation: `a(x) = x`).\n",
            " |    use_bias: Boolean (default `True`), whether the layer uses a bias vector.\n",
            " |    kernel_initializer: Initializer for the `kernel` weights matrix, used for\n",
            " |      the linear transformation of the inputs. Default: `glorot_uniform`.\n",
            " |    recurrent_initializer: Initializer for the `recurrent_kernel` weights\n",
            " |      matrix, used for the linear transformation of the recurrent state.\n",
            " |      Default: `orthogonal`.\n",
            " |    bias_initializer: Initializer for the bias vector. Default: `zeros`.\n",
            " |    unit_forget_bias: Boolean (default `True`). If True, add 1 to the bias of\n",
            " |      the forget gate at initialization. Setting it to true will also force\n",
            " |      `bias_initializer=\"zeros\"`. This is recommended in [Jozefowicz et\n",
            " |          al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n",
            " |    kernel_regularizer: Regularizer function applied to the `kernel` weights\n",
            " |      matrix. Default: `None`.\n",
            " |    recurrent_regularizer: Regularizer function applied to the\n",
            " |      `recurrent_kernel` weights matrix. Default: `None`.\n",
            " |    bias_regularizer: Regularizer function applied to the bias vector. Default:\n",
            " |      `None`.\n",
            " |    activity_regularizer: Regularizer function applied to the output of the\n",
            " |      layer (its \"activation\"). Default: `None`.\n",
            " |    kernel_constraint: Constraint function applied to the `kernel` weights\n",
            " |      matrix. Default: `None`.\n",
            " |    recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n",
            " |      weights matrix. Default: `None`.\n",
            " |    bias_constraint: Constraint function applied to the bias vector. Default:\n",
            " |      `None`.\n",
            " |    dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n",
            " |      transformation of the inputs. Default: 0.\n",
            " |    recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n",
            " |      the linear transformation of the recurrent state. Default: 0.\n",
            " |    return_sequences: Boolean. Whether to return the last output. in the output\n",
            " |      sequence, or the full sequence. Default: `False`.\n",
            " |    return_state: Boolean. Whether to return the last state in addition to the\n",
            " |      output. Default: `False`.\n",
            " |    go_backwards: Boolean (default `False`). If True, process the input sequence\n",
            " |      backwards and return the reversed sequence.\n",
            " |    stateful: Boolean (default `False`). If True, the last state for each sample\n",
            " |      at index i in a batch will be used as initial state for the sample of\n",
            " |      index i in the following batch.\n",
            " |    time_major: The shape format of the `inputs` and `outputs` tensors.\n",
            " |      If True, the inputs and outputs will be in shape\n",
            " |      `[timesteps, batch, feature]`, whereas in the False case, it will be\n",
            " |      `[batch, timesteps, feature]`. Using `time_major = True` is a bit more\n",
            " |      efficient because it avoids transposes at the beginning and end of the\n",
            " |      RNN calculation. However, most TensorFlow data is batch-major, so by\n",
            " |      default this function accepts input and emits output in batch-major\n",
            " |      form.\n",
            " |    unroll: Boolean (default `False`). If True, the network will be unrolled,\n",
            " |      else a symbolic loop will be used. Unrolling can speed-up a RNN, although\n",
            " |      it tends to be more memory-intensive. Unrolling is only suitable for short\n",
            " |      sequences.\n",
            " |  \n",
            " |  Call arguments:\n",
            " |    inputs: A 3D tensor with shape `[batch, timesteps, feature]`.\n",
            " |    mask: Binary tensor of shape `[batch, timesteps]` indicating whether\n",
            " |      a given timestep should be masked (optional, defaults to `None`).\n",
            " |      An individual `True` entry indicates that the corresponding timestep\n",
            " |      should be utilized, while a `False` entry indicates that the corresponding\n",
            " |      timestep should be ignored.\n",
            " |    training: Python boolean indicating whether the layer should behave in\n",
            " |      training mode or in inference mode. This argument is passed to the cell\n",
            " |      when calling it. This is only relevant if `dropout` or\n",
            " |      `recurrent_dropout` is used (optional, defaults to `None`).\n",
            " |    initial_state: List of initial state tensors to be passed to the first\n",
            " |      call of the cell (optional, defaults to `None` which causes creation\n",
            " |      of zero-filled initial state tensors).\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      LSTM\n",
            " |      keras.layers.recurrent.DropoutRNNCellMixin\n",
            " |      keras.layers.recurrent.LSTM\n",
            " |      keras.layers.recurrent.RNN\n",
            " |      keras.engine.base_layer.BaseRandomLayer\n",
            " |      keras.engine.base_layer.Layer\n",
            " |      tensorflow.python.module.module.Module\n",
            " |      tensorflow.python.training.tracking.autotrackable.AutoTrackable\n",
            " |      tensorflow.python.training.tracking.base.Trackable\n",
            " |      keras.utils.version_utils.LayerVersionSelector\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, go_backwards=False, stateful=False, time_major=False, unroll=False, **kwargs)\n",
            " |      Initialize the BaseRandomLayer.\n",
            " |      \n",
            " |      Note that the constructor is annotated with\n",
            " |      @no_automatic_dependency_tracking. This is to skip the auto\n",
            " |      tracking of self._random_generator instance, which is an AutoTrackable.\n",
            " |      The backend.RandomGenerator could contain a tf.random.Generator instance\n",
            " |      which will have tf.Variable as the internal state. We want to avoid saving\n",
            " |      that state into model.weights and checkpoints for backward compatibility\n",
            " |      reason. In the meantime, we still need to make them visible to SavedModel\n",
            " |      when it is tracing the tf.function for the `call()`.\n",
            " |      See _list_extra_dependencies_for_serialization below for more details.\n",
            " |      \n",
            " |      Args:\n",
            " |        seed: optional integer, used to create RandomGenerator.\n",
            " |        force_generator: boolean, default to False, whether to force the\n",
            " |          RandomGenerator to use the code branch of tf.random.Generator.\n",
            " |        **kwargs: other keyword arguments that will be passed to the parent class\n",
            " |  \n",
            " |  call(self, inputs, mask=None, training=None, initial_state=None)\n",
            " |      This is where the layer's logic lives.\n",
            " |      \n",
            " |      The `call()` method may not create state (except in its first invocation,\n",
            " |      wrapping the creation of variables or other resources in `tf.init_scope()`).\n",
            " |      It is recommended to create state in `__init__()`, or the `build()` method\n",
            " |      that is called automatically before `call()` executes the first time.\n",
            " |      \n",
            " |      Args:\n",
            " |        inputs: Input tensor, or dict/list/tuple of input tensors.\n",
            " |          The first positional `inputs` argument is subject to special rules:\n",
            " |          - `inputs` must be explicitly passed. A layer cannot have zero\n",
            " |            arguments, and `inputs` cannot be provided via the default value\n",
            " |            of a keyword argument.\n",
            " |          - NumPy array or Python scalar values in `inputs` get cast as tensors.\n",
            " |          - Keras mask metadata is only collected from `inputs`.\n",
            " |          - Layers are built (`build(input_shape)` method)\n",
            " |            using shape info from `inputs` only.\n",
            " |          - `input_spec` compatibility is only checked against `inputs`.\n",
            " |          - Mixed precision input casting is only applied to `inputs`.\n",
            " |            If a layer has tensor arguments in `*args` or `**kwargs`, their\n",
            " |            casting behavior in mixed precision should be handled manually.\n",
            " |          - The SavedModel input specification is generated using `inputs` only.\n",
            " |          - Integration with various ecosystem packages like TFMOT, TFLite,\n",
            " |            TF.js, etc is only supported for `inputs` and not for tensors in\n",
            " |            positional and keyword arguments.\n",
            " |        *args: Additional positional arguments. May contain tensors, although\n",
            " |          this is not recommended, for the reasons above.\n",
            " |        **kwargs: Additional keyword arguments. May contain tensors, although\n",
            " |          this is not recommended, for the reasons above.\n",
            " |          The following optional keyword arguments are reserved:\n",
            " |          - `training`: Boolean scalar tensor of Python boolean indicating\n",
            " |            whether the `call` is meant for training or inference.\n",
            " |          - `mask`: Boolean input mask. If the layer's `call()` method takes a\n",
            " |            `mask` argument, its default value will be set to the mask generated\n",
            " |            for `inputs` by the previous layer (if `input` did come from a layer\n",
            " |            that generated a corresponding mask, i.e. if it came from a Keras\n",
            " |            layer with masking support).\n",
            " |      \n",
            " |      Returns:\n",
            " |        A tensor or list/tuple of tensors.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.layers.recurrent.DropoutRNNCellMixin:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_dropout_mask_for_cell(self, inputs, training, count=1)\n",
            " |      Get the dropout mask for RNN cell's input.\n",
            " |      \n",
            " |      It will create mask based on context if there isn't any existing cached\n",
            " |      mask. If a new mask is generated, it will update the cache in the cell.\n",
            " |      \n",
            " |      Args:\n",
            " |        inputs: The input tensor whose shape will be used to generate dropout\n",
            " |          mask.\n",
            " |        training: Boolean tensor, whether its in training mode, dropout will be\n",
            " |          ignored in non-training mode.\n",
            " |        count: Int, how many dropout mask will be generated. It is useful for cell\n",
            " |          that has internal weights fused together.\n",
            " |      Returns:\n",
            " |        List of mask tensor, generated or cached mask based on context.\n",
            " |  \n",
            " |  get_recurrent_dropout_mask_for_cell(self, inputs, training, count=1)\n",
            " |      Get the recurrent dropout mask for RNN cell.\n",
            " |      \n",
            " |      It will create mask based on context if there isn't any existing cached\n",
            " |      mask. If a new mask is generated, it will update the cache in the cell.\n",
            " |      \n",
            " |      Args:\n",
            " |        inputs: The input tensor whose shape will be used to generate dropout\n",
            " |          mask.\n",
            " |        training: Boolean tensor, whether its in training mode, dropout will be\n",
            " |          ignored in non-training mode.\n",
            " |        count: Int, how many dropout mask will be generated. It is useful for cell\n",
            " |          that has internal weights fused together.\n",
            " |      Returns:\n",
            " |        List of mask tensor, generated or cached mask based on context.\n",
            " |  \n",
            " |  reset_dropout_mask(self)\n",
            " |      Reset the cached dropout masks if any.\n",
            " |      \n",
            " |      This is important for the RNN layer to invoke this in it `call()` method so\n",
            " |      that the cached mask is cleared before calling the `cell.call()`. The mask\n",
            " |      should be cached across the timestep within the same batch, but shouldn't\n",
            " |      be cached between batches. Otherwise it will introduce unreasonable bias\n",
            " |      against certain index of data within the batch.\n",
            " |  \n",
            " |  reset_recurrent_dropout_mask(self)\n",
            " |      Reset the cached recurrent dropout masks if any.\n",
            " |      \n",
            " |      This is important for the RNN layer to invoke this in it call() method so\n",
            " |      that the cached mask is cleared before calling the cell.call(). The mask\n",
            " |      should be cached across the timestep within the same batch, but shouldn't\n",
            " |      be cached between batches. Otherwise it will introduce unreasonable bias\n",
            " |      against certain index of data within the batch.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from keras.layers.recurrent.DropoutRNNCellMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.layers.recurrent.LSTM:\n",
            " |  \n",
            " |  get_config(self)\n",
            " |      Returns the config of the layer.\n",
            " |      \n",
            " |      A layer config is a Python dictionary (serializable)\n",
            " |      containing the configuration of a layer.\n",
            " |      The same layer can be reinstantiated later\n",
            " |      (without its trained weights) from this configuration.\n",
            " |      \n",
            " |      The config of a layer does not include connectivity\n",
            " |      information, nor the layer class name. These are handled\n",
            " |      by `Network` (one layer of abstraction above).\n",
            " |      \n",
            " |      Note that `get_config()` does not guarantee to return a fresh copy of dict\n",
            " |      every time it is called. The callers should make a copy of the returned dict\n",
            " |      if they want to modify it.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Python dictionary.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from keras.layers.recurrent.LSTM:\n",
            " |  \n",
            " |  from_config(config) from builtins.type\n",
            " |      Creates a layer from its config.\n",
            " |      \n",
            " |      This method is the reverse of `get_config`,\n",
            " |      capable of instantiating the same layer from the config\n",
            " |      dictionary. It does not handle layer connectivity\n",
            " |      (handled by Network), nor weights (handled by `set_weights`).\n",
            " |      \n",
            " |      Args:\n",
            " |          config: A Python dictionary, typically the\n",
            " |              output of get_config.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A layer instance.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from keras.layers.recurrent.LSTM:\n",
            " |  \n",
            " |  activation\n",
            " |  \n",
            " |  bias_constraint\n",
            " |  \n",
            " |  bias_initializer\n",
            " |  \n",
            " |  bias_regularizer\n",
            " |  \n",
            " |  dropout\n",
            " |  \n",
            " |  implementation\n",
            " |  \n",
            " |  kernel_constraint\n",
            " |  \n",
            " |  kernel_initializer\n",
            " |  \n",
            " |  kernel_regularizer\n",
            " |  \n",
            " |  recurrent_activation\n",
            " |  \n",
            " |  recurrent_constraint\n",
            " |  \n",
            " |  recurrent_dropout\n",
            " |  \n",
            " |  recurrent_initializer\n",
            " |  \n",
            " |  recurrent_regularizer\n",
            " |  \n",
            " |  unit_forget_bias\n",
            " |  \n",
            " |  units\n",
            " |  \n",
            " |  use_bias\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.layers.recurrent.RNN:\n",
            " |  \n",
            " |  __call__(self, inputs, initial_state=None, constants=None, **kwargs)\n",
            " |      Wraps `call`, applying pre- and post-processing steps.\n",
            " |      \n",
            " |      Args:\n",
            " |        *args: Positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |      \n",
            " |      Note:\n",
            " |        - The following optional keyword arguments are reserved for specific uses:\n",
            " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
            " |            whether the `call` is meant for training or inference.\n",
            " |          * `mask`: Boolean input mask.\n",
            " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
            " |          layers do), its default value will be set to the mask generated\n",
            " |          for `inputs` by the previous layer (if `input` did come from\n",
            " |          a layer that generated a corresponding mask, i.e. if it came from\n",
            " |          a Keras layer with masking support.\n",
            " |        - If the layer is not built, the method will call `build`.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
            " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
            " |  \n",
            " |  build(self, input_shape)\n",
            " |      Creates the variables of the layer (optional, for subclass implementers).\n",
            " |      \n",
            " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
            " |      can override if they need a state-creation step in-between\n",
            " |      layer instantiation and layer call. It is invoked automatically before\n",
            " |      the first execution of `call()`.\n",
            " |      \n",
            " |      This is typically used to create the weights of `Layer` subclasses\n",
            " |      (at the discretion of the subclass implementer).\n",
            " |      \n",
            " |      Args:\n",
            " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
            " |          `TensorShape` if the layer expects a list of inputs\n",
            " |          (one instance per input).\n",
            " |  \n",
            " |  compute_mask(self, inputs, mask)\n",
            " |      Computes an output mask tensor.\n",
            " |      \n",
            " |      Args:\n",
            " |          inputs: Tensor or list of tensors.\n",
            " |          mask: Tensor or list of tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |          None or a tensor (or list of tensors,\n",
            " |              one per output tensor of the layer).\n",
            " |  \n",
            " |  compute_output_shape(self, input_shape)\n",
            " |      Computes the output shape of the layer.\n",
            " |      \n",
            " |      This method will cause the layer's state to be built, if that has not\n",
            " |      happened before. This requires that the layer will later be used with\n",
            " |      inputs that match the input shape provided here.\n",
            " |      \n",
            " |      Args:\n",
            " |          input_shape: Shape tuple (tuple of integers)\n",
            " |              or list of shape tuples (one per output tensor of the layer).\n",
            " |              Shape tuples can include None for free dimensions,\n",
            " |              instead of an integer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An input shape tuple.\n",
            " |  \n",
            " |  get_initial_state(self, inputs)\n",
            " |  \n",
            " |  reset_states(self, states=None)\n",
            " |      Reset the recorded states for the stateful RNN layer.\n",
            " |      \n",
            " |      Can only be used when RNN layer is constructed with `stateful` = `True`.\n",
            " |      Args:\n",
            " |        states: Numpy arrays that contains the value for the initial state, which\n",
            " |          will be feed to cell at the first time step. When the value is None,\n",
            " |          zero filled numpy array will be created based on the cell state size.\n",
            " |      \n",
            " |      Raises:\n",
            " |        AttributeError: When the RNN layer is not stateful.\n",
            " |        ValueError: When the batch size of the RNN layer is unknown.\n",
            " |        ValueError: When the input numpy array is not compatible with the RNN\n",
            " |          layer state, either size wise or dtype wise.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from keras.layers.recurrent.RNN:\n",
            " |  \n",
            " |  states\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  __delattr__(self, name)\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Support self.foo = trackable syntax.\n",
            " |  \n",
            " |  add_loss(self, losses, **kwargs)\n",
            " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Some losses (for instance, activity regularization losses) may be dependent\n",
            " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
            " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
            " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      This method can be used inside a subclassed layer or model's `call`\n",
            " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyLayer(tf.keras.layers.Layer):\n",
            " |        def call(self, inputs):\n",
            " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      This method can also be called directly on a Functional Model during\n",
            " |      construction. In this case, any loss Tensors passed to this Model must\n",
            " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
            " |      losses become part of the model's topology and are tracked in `get_config`.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Activity regularization.\n",
            " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      ```\n",
            " |      \n",
            " |      If this is not the case for your loss (if, for example, your loss references\n",
            " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
            " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
            " |      topology since they can't be serialized.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      d = tf.keras.layers.Dense(10)\n",
            " |      x = d(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      # Weight regularization.\n",
            " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
            " |          may also be zero-argument callables which create a loss tensor.\n",
            " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
            " |          Accepted values:\n",
            " |            inputs - Deprecated, will be automatically inferred.\n",
            " |  \n",
            " |  add_metric(self, value, name=None, **kwargs)\n",
            " |      Adds metric tensor to the layer.\n",
            " |      \n",
            " |      This method can be used inside the `call()` method of a subclassed layer\n",
            " |      or model.\n",
            " |      \n",
            " |      ```python\n",
            " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
            " |        def __init__(self):\n",
            " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
            " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
            " |      \n",
            " |        def call(self, inputs):\n",
            " |          self.add_metric(self.mean(inputs))\n",
            " |          self.add_metric(tf.reduce_sum(inputs), name='metric_2')\n",
            " |          return inputs\n",
            " |      ```\n",
            " |      \n",
            " |      This method can also be called directly on a Functional Model during\n",
            " |      construction. In this case, any tensor passed to this Model must\n",
            " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
            " |      metrics become part of the model's topology and are tracked when you\n",
            " |      save the model via `save()`.\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
            " |      ```\n",
            " |      \n",
            " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
            " |      Functional Model, as shown in the example below, is not supported. This is\n",
            " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
            " |      \n",
            " |      ```python\n",
            " |      inputs = tf.keras.Input(shape=(10,))\n",
            " |      x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      model = tf.keras.Model(inputs, outputs)\n",
            " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        value: Metric tensor.\n",
            " |        name: String metric name.\n",
            " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
            " |          Accepted values:\n",
            " |          `aggregation` - When the `value` tensor provided is not the result of\n",
            " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
            " |          using a `keras.Metric.Mean`.\n",
            " |  \n",
            " |  add_update(self, updates, inputs=None)\n",
            " |      Add update op(s), potentially dependent on layer inputs.\n",
            " |      \n",
            " |      Weight updates (for instance, the updates of the moving mean and variance\n",
            " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
            " |      when calling a layer. Hence, when reusing the same layer on\n",
            " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
            " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
            " |      of dependencies.\n",
            " |      \n",
            " |      This call is ignored when eager execution is enabled (in that case, variable\n",
            " |      updates are run on the fly and thus do not need to be tracked for later\n",
            " |      execution).\n",
            " |      \n",
            " |      Args:\n",
            " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
            " |          that returns an update op. A zero-arg callable should be passed in\n",
            " |          order to disable running the updates by setting `trainable=False`\n",
            " |          on this Layer, when executing in Eager mode.\n",
            " |        inputs: Deprecated, will be automatically inferred.\n",
            " |  \n",
            " |  add_variable(self, *args, **kwargs)\n",
            " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
            " |  \n",
            " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregationV2.NONE: 0>, **kwargs)\n",
            " |      Adds a new variable to the layer.\n",
            " |      \n",
            " |      Args:\n",
            " |        name: Variable name.\n",
            " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
            " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
            " |        initializer: Initializer instance (callable).\n",
            " |        regularizer: Regularizer instance (callable).\n",
            " |        trainable: Boolean, whether the variable should be part of the layer's\n",
            " |          \"trainable_variables\" (e.g. variables, biases)\n",
            " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
            " |          Note that `trainable` cannot be `True` if `synchronization`\n",
            " |          is set to `ON_READ`.\n",
            " |        constraint: Constraint instance (callable).\n",
            " |        use_resource: Whether to use `ResourceVariable`.\n",
            " |        synchronization: Indicates when a distributed a variable will be\n",
            " |          aggregated. Accepted values are constants defined in the class\n",
            " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
            " |          `AUTO` and the current `DistributionStrategy` chooses\n",
            " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
            " |          `trainable` must not be set to `True`.\n",
            " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
            " |          Accepted values are constants defined in the class\n",
            " |          `tf.VariableAggregation`.\n",
            " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
            " |          `collections`, `experimental_autocast` and `caching_device`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The variable created.\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: When giving unsupported dtype and no initializer or when\n",
            " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
            " |  \n",
            " |  apply(self, inputs, *args, **kwargs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      This is an alias of `self.__call__`.\n",
            " |      \n",
            " |      Args:\n",
            " |        inputs: Input tensor(s).\n",
            " |        *args: additional positional arguments to be passed to `self.call`.\n",
            " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor(s).\n",
            " |  \n",
            " |  compute_output_signature(self, input_signature)\n",
            " |      Compute the output tensor signature of the layer based on the inputs.\n",
            " |      \n",
            " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
            " |      and dtype information for a tensor. This method allows layers to provide\n",
            " |      output dtype information if it is different from the input dtype.\n",
            " |      For any layer that doesn't implement this function,\n",
            " |      the framework will fall back to use `compute_output_shape`, and will\n",
            " |      assume that the output dtype matches the input dtype.\n",
            " |      \n",
            " |      Args:\n",
            " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
            " |          objects, describing a candidate input for the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
            " |          how the layer would transform the provided input.\n",
            " |      \n",
            " |      Raises:\n",
            " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
            " |  \n",
            " |  count_params(self)\n",
            " |      Count the total number of scalars composing the weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |          An integer count.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: if the layer isn't yet built\n",
            " |            (in which case its weights aren't yet defined).\n",
            " |  \n",
            " |  finalize_state(self)\n",
            " |      Finalizes the layers state after updating layer weights.\n",
            " |      \n",
            " |      This function can be subclassed in a layer and will be called after updating\n",
            " |      a layer weights. It can be overridden to finalize any additional layer state\n",
            " |      after a weight update.\n",
            " |      \n",
            " |      This function will be called after weights of a layer have been restored\n",
            " |      from a loaded model.\n",
            " |  \n",
            " |  get_input_at(self, node_index)\n",
            " |      Retrieves the input tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first input node of the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_input_mask_at(self, node_index)\n",
            " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_input_shape_at(self, node_index)\n",
            " |      Retrieves the input shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple inputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_losses_for(self, inputs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Retrieves losses relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Args:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of loss tensors of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  get_output_at(self, node_index)\n",
            " |      Retrieves the output tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first output node of the layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_output_mask_at(self, node_index)\n",
            " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_output_shape_at(self, node_index)\n",
            " |      Retrieves the output shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      Args:\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple outputs).\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |  \n",
            " |  get_updates_for(self, inputs)\n",
            " |      Deprecated, do NOT use!\n",
            " |      \n",
            " |      Retrieves updates relevant to a specific set of inputs.\n",
            " |      \n",
            " |      Args:\n",
            " |        inputs: Input tensor or list/tuple of input tensors.\n",
            " |      \n",
            " |      Returns:\n",
            " |        List of update ops of the layer that depend on `inputs`.\n",
            " |  \n",
            " |  get_weights(self)\n",
            " |      Returns the current weights of the layer, as NumPy arrays.\n",
            " |      \n",
            " |      The weights of a layer represent the state of the layer. This function\n",
            " |      returns both trainable and non-trainable weight values associated with this\n",
            " |      layer as a list of NumPy arrays, which can in turn be used to load state\n",
            " |      into similarly parameterized layers.\n",
            " |      \n",
            " |      For example, a `Dense` layer returns a list of two values: the kernel matrix\n",
            " |      and the bias vector. These can be used to set the weights of another\n",
            " |      `Dense` layer:\n",
            " |      \n",
            " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
            " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
            " |      >>> layer_a.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
            " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
            " |      >>> layer_b.get_weights()\n",
            " |      [array([[2.],\n",
            " |             [2.],\n",
            " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
            " |      >>> layer_b.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      \n",
            " |      Returns:\n",
            " |          Weights values as a list of NumPy arrays.\n",
            " |  \n",
            " |  set_weights(self, weights)\n",
            " |      Sets the weights of the layer, from NumPy arrays.\n",
            " |      \n",
            " |      The weights of a layer represent the state of the layer. This function\n",
            " |      sets the weight values from numpy arrays. The weight values should be\n",
            " |      passed in the order they are created by the layer. Note that the layer's\n",
            " |      weights must be instantiated before calling this function, by calling\n",
            " |      the layer.\n",
            " |      \n",
            " |      For example, a `Dense` layer returns a list of two values: the kernel matrix\n",
            " |      and the bias vector. These can be used to set the weights of another\n",
            " |      `Dense` layer:\n",
            " |      \n",
            " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
            " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
            " |      >>> layer_a.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
            " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
            " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
            " |      >>> layer_b.get_weights()\n",
            " |      [array([[2.],\n",
            " |             [2.],\n",
            " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
            " |      >>> layer_b.get_weights()\n",
            " |      [array([[1.],\n",
            " |             [1.],\n",
            " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
            " |      \n",
            " |      Args:\n",
            " |        weights: a list of NumPy arrays. The number\n",
            " |          of arrays and their shape must match\n",
            " |          number of the dimensions of the weights\n",
            " |          of the layer (i.e. it should match the\n",
            " |          output of `get_weights`).\n",
            " |      \n",
            " |      Raises:\n",
            " |        ValueError: If the provided weights list does not match the\n",
            " |          layer's specifications.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  activity_regularizer\n",
            " |      Optional regularizer function for the output of this layer.\n",
            " |  \n",
            " |  compute_dtype\n",
            " |      The dtype of the layer's computations.\n",
            " |      \n",
            " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
            " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
            " |      the weights.\n",
            " |      \n",
            " |      Layers automatically cast their inputs to the compute dtype, which causes\n",
            " |      computations and the output to be in the compute dtype as well. This is done\n",
            " |      by the base Layer class in `Layer.__call__`, so you do not have to insert\n",
            " |      these casts if implementing your own layer.\n",
            " |      \n",
            " |      Layers often perform certain internal computations in higher precision when\n",
            " |      `compute_dtype` is float16 or bfloat16 for numeric stability. The output\n",
            " |      will still typically be float16 or bfloat16 in such cases.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The layer's compute dtype.\n",
            " |  \n",
            " |  dtype\n",
            " |      The dtype of the layer weights.\n",
            " |      \n",
            " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
            " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
            " |      dtype of the layer's computations.\n",
            " |  \n",
            " |  dtype_policy\n",
            " |      The dtype policy associated with this layer.\n",
            " |      \n",
            " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
            " |  \n",
            " |  dynamic\n",
            " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
            " |  \n",
            " |  inbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  input\n",
            " |      Retrieves the input tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input tensor or list of input tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        RuntimeError: If called in Eager mode.\n",
            " |        AttributeError: If no inbound nodes are found.\n",
            " |  \n",
            " |  input_mask\n",
            " |      Retrieves the input mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input mask tensor (potentially None) or list of input\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  input_shape\n",
            " |      Retrieves the input shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one input,\n",
            " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
            " |      have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Input shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per input tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined input_shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  input_spec\n",
            " |      `InputSpec` instance(s) describing the input format for this layer.\n",
            " |      \n",
            " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
            " |      the layer to run input compatibility checks when it is called.\n",
            " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
            " |      of rank 4. As such, you can set, in `__init__()`:\n",
            " |      \n",
            " |      ```python\n",
            " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
            " |      ```\n",
            " |      \n",
            " |      Now, if you try to call the layer on an input that isn't rank 4\n",
            " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
            " |      error:\n",
            " |      \n",
            " |      ```\n",
            " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
            " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
            " |      ```\n",
            " |      \n",
            " |      Input checks that can be specified via `input_spec` include:\n",
            " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
            " |      - Shape\n",
            " |      - Rank (ndim)\n",
            " |      - Dtype\n",
            " |      \n",
            " |      For more information, see `tf.keras.layers.InputSpec`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
            " |  \n",
            " |  losses\n",
            " |      List of losses added using the `add_loss()` API.\n",
            " |      \n",
            " |      Variable regularization tensors are created when this property is accessed,\n",
            " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
            " |      propagate gradients back to the corresponding variables.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
            " |      ...   def call(self, inputs):\n",
            " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
            " |      ...     return inputs\n",
            " |      >>> l = MyLayer()\n",
            " |      >>> l(np.ones((10, 1)))\n",
            " |      >>> l.losses\n",
            " |      [1.0]\n",
            " |      \n",
            " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
            " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
            " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      >>> model = tf.keras.Model(inputs, outputs)\n",
            " |      >>> # Activity regularization.\n",
            " |      >>> len(model.losses)\n",
            " |      0\n",
            " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
            " |      >>> len(model.losses)\n",
            " |      1\n",
            " |      \n",
            " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
            " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
            " |      >>> x = d(inputs)\n",
            " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
            " |      >>> model = tf.keras.Model(inputs, outputs)\n",
            " |      >>> # Weight regularization.\n",
            " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
            " |      >>> model.losses\n",
            " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of tensors.\n",
            " |  \n",
            " |  metrics\n",
            " |      List of metrics added using the `add_metric()` API.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      >>> input = tf.keras.layers.Input(shape=(3,))\n",
            " |      >>> d = tf.keras.layers.Dense(2)\n",
            " |      >>> output = d(input)\n",
            " |      >>> d.add_metric(tf.reduce_max(output), name='max')\n",
            " |      >>> d.add_metric(tf.reduce_min(output), name='min')\n",
            " |      >>> [m.name for m in d.metrics]\n",
            " |      ['max', 'min']\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of `Metric` objects.\n",
            " |  \n",
            " |  name\n",
            " |      Name of the layer (string), set in the constructor.\n",
            " |  \n",
            " |  non_trainable_variables\n",
            " |      Sequence of non-trainable variables owned by this module and its submodules.\n",
            " |      \n",
            " |      Note: this method uses reflection to find variables on the current instance\n",
            " |      and submodules. For performance reasons you may wish to cache the result\n",
            " |      of calling this method if you don't expect the return value to change.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of variables for the current module (sorted by attribute\n",
            " |        name) followed by variables from all submodules recursively (breadth\n",
            " |        first).\n",
            " |  \n",
            " |  non_trainable_weights\n",
            " |      List of all non-trainable weights tracked by this layer.\n",
            " |      \n",
            " |      Non-trainable weights are *not* updated during training. They are expected\n",
            " |      to be updated manually in `call()`.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of non-trainable variables.\n",
            " |  \n",
            " |  outbound_nodes\n",
            " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
            " |  \n",
            " |  output\n",
            " |      Retrieves the output tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one output,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |        Output tensor or list of output tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |        AttributeError: if the layer is connected to more than one incoming\n",
            " |          layers.\n",
            " |        RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  output_mask\n",
            " |      Retrieves the output mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output mask tensor (potentially None) or list of output\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output_shape\n",
            " |      Retrieves the output shape(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has one output,\n",
            " |      or if all outputs have the same shape.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Output shape, as an integer shape tuple\n",
            " |          (or list of shape tuples, one tuple per output tensor).\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: if the layer has no defined output shape.\n",
            " |          RuntimeError: if called in Eager mode.\n",
            " |  \n",
            " |  stateful\n",
            " |  \n",
            " |  supports_masking\n",
            " |      Whether this layer supports computing a mask using `compute_mask`.\n",
            " |  \n",
            " |  trainable\n",
            " |  \n",
            " |  trainable_variables\n",
            " |      Sequence of trainable variables owned by this module and its submodules.\n",
            " |      \n",
            " |      Note: this method uses reflection to find variables on the current instance\n",
            " |      and submodules. For performance reasons you may wish to cache the result\n",
            " |      of calling this method if you don't expect the return value to change.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of variables for the current module (sorted by attribute\n",
            " |        name) followed by variables from all submodules recursively (breadth\n",
            " |        first).\n",
            " |  \n",
            " |  trainable_weights\n",
            " |      List of all trainable weights tracked by this layer.\n",
            " |      \n",
            " |      Trainable weights are updated via gradient descent during training.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of trainable variables.\n",
            " |  \n",
            " |  updates\n",
            " |  \n",
            " |  variable_dtype\n",
            " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
            " |  \n",
            " |  variables\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Alias of `self.weights`.\n",
            " |      \n",
            " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
            " |      themselves Keras layers.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  weights\n",
            " |      Returns the list of all layer variables/weights.\n",
            " |      \n",
            " |      Returns:\n",
            " |        A list of variables.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  with_name_scope(method) from builtins.type\n",
            " |      Decorator to automatically enter the module name scope.\n",
            " |      \n",
            " |      >>> class MyModule(tf.Module):\n",
            " |      ...   @tf.Module.with_name_scope\n",
            " |      ...   def __call__(self, x):\n",
            " |      ...     if not hasattr(self, 'w'):\n",
            " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
            " |      ...     return tf.matmul(x, self.w)\n",
            " |      \n",
            " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
            " |      names included the module name:\n",
            " |      \n",
            " |      >>> mod = MyModule()\n",
            " |      >>> mod(tf.ones([1, 2]))\n",
            " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
            " |      >>> mod.w\n",
            " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
            " |      numpy=..., dtype=float32)>\n",
            " |      \n",
            " |      Args:\n",
            " |        method: The method to wrap.\n",
            " |      \n",
            " |      Returns:\n",
            " |        The original method wrapped such that it enters the module's name scope.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
            " |  \n",
            " |  name_scope\n",
            " |      Returns a `tf.name_scope` instance for this class.\n",
            " |  \n",
            " |  submodules\n",
            " |      Sequence of all sub-modules.\n",
            " |      \n",
            " |      Submodules are modules which are properties of this module, or found as\n",
            " |      properties of modules which are properties of this module (and so on).\n",
            " |      \n",
            " |      >>> a = tf.Module()\n",
            " |      >>> b = tf.Module()\n",
            " |      >>> c = tf.Module()\n",
            " |      >>> a.b = b\n",
            " |      >>> b.c = c\n",
            " |      >>> list(a.submodules) == [b, c]\n",
            " |      True\n",
            " |      >>> list(b.submodules) == [c]\n",
            " |      True\n",
            " |      >>> list(c.submodules) == []\n",
            " |      True\n",
            " |      \n",
            " |      Returns:\n",
            " |        A sequence of all submodules.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods inherited from keras.utils.version_utils.LayerVersionSelector:\n",
            " |  \n",
            " |  __new__(cls, *args, **kwargs)\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.add(LSTM(units=50,return_sequences=True,input_shape=(X_train.shape[1],1)))\n",
        "regressor.add(Dropout(0.2)) \n",
        "# Adding a second layer\n",
        "regressor.add(LSTM(units = 50, return_sequences = True))  \n",
        "regressor.add(Dropout(0.2)) \n",
        "# Adding a third LSTM layer and some Dropout regularization  \n",
        "regressor.add(LSTM(units = 50, return_sequences = True))  \n",
        "regressor.add(Dropout(0.2)) \n",
        "# Adding a fourth LSTM layer and some Dropout regularization  \n",
        "regressor.add(LSTM(units = 50))  \n",
        "regressor.add(Dropout(0.2))"
      ],
      "metadata": {
        "id": "fBD7dKBS-4LC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.add(Dense(units=1))"
      ],
      "metadata": {
        "id": "mB8UOdFPKAms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.compile(optimizer='adam',loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "kjMt1nGYPKLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the RNN to the Training set  \n",
        "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpO8SC7XPdZi",
        "outputId": "1471d6d7-a7bc-4e2a-f26c-8bc868cd87f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "38/38 [==============================] - 12s 115ms/step - loss: 0.0442\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.0060\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 5s 135ms/step - loss: 0.0057\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.0051\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 4s 115ms/step - loss: 0.0050\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.0051\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.0045\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 4s 115ms/step - loss: 0.0050\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.0042\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 4s 118ms/step - loss: 0.0043\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.0042\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 4s 115ms/step - loss: 0.0043\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 4s 115ms/step - loss: 0.0040\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 4s 115ms/step - loss: 0.0039\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 4s 116ms/step - loss: 0.0043\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 4s 115ms/step - loss: 0.0036\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.0035\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0039\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.0037\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 5s 121ms/step - loss: 0.0038\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.0038\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.0033\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 4s 115ms/step - loss: 0.0032\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 4s 108ms/step - loss: 0.0041\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.0034\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.0031\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.0032\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.0030\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.0036\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0031\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 4s 108ms/step - loss: 0.0029\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.0029\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 4s 111ms/step - loss: 0.0030\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 4s 108ms/step - loss: 0.0027\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 4s 111ms/step - loss: 0.0028\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.0028\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 4s 108ms/step - loss: 0.0026\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0033\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0026\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0026\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.0027\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.0025\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.0029\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.0030\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0023\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.0024\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0023\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.0023\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.0024\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.0023\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 4s 108ms/step - loss: 0.0024\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 4s 113ms/step - loss: 0.0021\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.0025\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0027\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0021\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 4s 108ms/step - loss: 0.0021\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 4s 108ms/step - loss: 0.0023\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0023\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0022\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 4s 108ms/step - loss: 0.0020\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0022\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 4s 108ms/step - loss: 0.0021\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.0020\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.0021\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.0021\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.0021\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.0019\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.0019\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.0019\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.0020\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.0019\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0019\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0017\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0019\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.0016\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0018\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.0019\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.0018\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.0019\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.0018\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0017\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.0016\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 4s 114ms/step - loss: 0.0017\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.0016\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.0017\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0016\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.0016\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0015\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 4s 109ms/step - loss: 0.0017\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.0018\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 4s 106ms/step - loss: 0.0016\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0018\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0016\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 4s 115ms/step - loss: 0.0017\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0014\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 4s 110ms/step - loss: 0.0014\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 4s 107ms/step - loss: 0.0015\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 4s 112ms/step - loss: 0.0015\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 4s 108ms/step - loss: 0.0016\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 4s 108ms/step - loss: 0.0017\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8768ab9050>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Making predictions and visualizing the data\n"
      ],
      "metadata": {
        "id": "ddOOhzb0Ufi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the real stock price of 2017  \n",
        "dataset_test = pd.read_csv('/content/Google_Stock_Price_Test.csv')  \n",
        "real_stock_price = dataset_test.iloc[:, 1:2].values \n",
        "real_stock_price\n",
        "dataset_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "WpQ7aScpUtyi",
        "outputId": "607544c6-8942-4415-bad1-c669759d7efc"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Date    Open    High     Low   Close     Volume\n",
              "0    1/3/2017  778.81  789.63  775.80  786.14  1,657,300\n",
              "1    1/4/2017  788.36  791.34  783.16  786.90  1,073,000\n",
              "2    1/5/2017  786.08  794.48  785.02  794.02  1,335,200\n",
              "3    1/6/2017  795.26  807.90  792.20  806.15  1,640,200\n",
              "4    1/9/2017  806.40  809.97  802.83  806.65  1,272,400\n",
              "5   1/10/2017  807.86  809.13  803.51  804.79  1,176,800\n",
              "6   1/11/2017  805.00  808.15  801.37  807.91  1,065,900\n",
              "7   1/12/2017  807.14  807.39  799.17  806.36  1,353,100\n",
              "8   1/13/2017  807.48  811.22  806.69  807.88  1,099,200\n",
              "9   1/17/2017  807.08  807.14  800.37  804.61  1,362,100\n",
              "10  1/18/2017  805.81  806.21  800.99  806.07  1,294,400\n",
              "11  1/19/2017  805.12  809.48  801.80  802.17    919,300\n",
              "12  1/20/2017  806.91  806.91  801.69  805.02  1,670,000\n",
              "13  1/23/2017  807.25  820.87  803.74  819.31  1,963,600\n",
              "14  1/24/2017  822.30  825.90  817.82  823.87  1,474,000\n",
              "15  1/25/2017  829.62  835.77  825.06  835.67  1,494,500\n",
              "16  1/26/2017  837.81  838.00  827.01  832.15  2,973,900\n",
              "17  1/27/2017  834.71  841.95  820.44  823.31  2,965,800\n",
              "18  1/30/2017  814.66  815.84  799.80  802.32  3,246,600\n",
              "19  1/31/2017  796.86  801.25  790.52  796.79  2,160,600"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-565c886b-5f28-4bd4-b371-ac4835b6022b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1/3/2017</td>\n",
              "      <td>778.81</td>\n",
              "      <td>789.63</td>\n",
              "      <td>775.80</td>\n",
              "      <td>786.14</td>\n",
              "      <td>1,657,300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1/4/2017</td>\n",
              "      <td>788.36</td>\n",
              "      <td>791.34</td>\n",
              "      <td>783.16</td>\n",
              "      <td>786.90</td>\n",
              "      <td>1,073,000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1/5/2017</td>\n",
              "      <td>786.08</td>\n",
              "      <td>794.48</td>\n",
              "      <td>785.02</td>\n",
              "      <td>794.02</td>\n",
              "      <td>1,335,200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1/6/2017</td>\n",
              "      <td>795.26</td>\n",
              "      <td>807.90</td>\n",
              "      <td>792.20</td>\n",
              "      <td>806.15</td>\n",
              "      <td>1,640,200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1/9/2017</td>\n",
              "      <td>806.40</td>\n",
              "      <td>809.97</td>\n",
              "      <td>802.83</td>\n",
              "      <td>806.65</td>\n",
              "      <td>1,272,400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1/10/2017</td>\n",
              "      <td>807.86</td>\n",
              "      <td>809.13</td>\n",
              "      <td>803.51</td>\n",
              "      <td>804.79</td>\n",
              "      <td>1,176,800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1/11/2017</td>\n",
              "      <td>805.00</td>\n",
              "      <td>808.15</td>\n",
              "      <td>801.37</td>\n",
              "      <td>807.91</td>\n",
              "      <td>1,065,900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1/12/2017</td>\n",
              "      <td>807.14</td>\n",
              "      <td>807.39</td>\n",
              "      <td>799.17</td>\n",
              "      <td>806.36</td>\n",
              "      <td>1,353,100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1/13/2017</td>\n",
              "      <td>807.48</td>\n",
              "      <td>811.22</td>\n",
              "      <td>806.69</td>\n",
              "      <td>807.88</td>\n",
              "      <td>1,099,200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1/17/2017</td>\n",
              "      <td>807.08</td>\n",
              "      <td>807.14</td>\n",
              "      <td>800.37</td>\n",
              "      <td>804.61</td>\n",
              "      <td>1,362,100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1/18/2017</td>\n",
              "      <td>805.81</td>\n",
              "      <td>806.21</td>\n",
              "      <td>800.99</td>\n",
              "      <td>806.07</td>\n",
              "      <td>1,294,400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1/19/2017</td>\n",
              "      <td>805.12</td>\n",
              "      <td>809.48</td>\n",
              "      <td>801.80</td>\n",
              "      <td>802.17</td>\n",
              "      <td>919,300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1/20/2017</td>\n",
              "      <td>806.91</td>\n",
              "      <td>806.91</td>\n",
              "      <td>801.69</td>\n",
              "      <td>805.02</td>\n",
              "      <td>1,670,000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1/23/2017</td>\n",
              "      <td>807.25</td>\n",
              "      <td>820.87</td>\n",
              "      <td>803.74</td>\n",
              "      <td>819.31</td>\n",
              "      <td>1,963,600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1/24/2017</td>\n",
              "      <td>822.30</td>\n",
              "      <td>825.90</td>\n",
              "      <td>817.82</td>\n",
              "      <td>823.87</td>\n",
              "      <td>1,474,000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1/25/2017</td>\n",
              "      <td>829.62</td>\n",
              "      <td>835.77</td>\n",
              "      <td>825.06</td>\n",
              "      <td>835.67</td>\n",
              "      <td>1,494,500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1/26/2017</td>\n",
              "      <td>837.81</td>\n",
              "      <td>838.00</td>\n",
              "      <td>827.01</td>\n",
              "      <td>832.15</td>\n",
              "      <td>2,973,900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1/27/2017</td>\n",
              "      <td>834.71</td>\n",
              "      <td>841.95</td>\n",
              "      <td>820.44</td>\n",
              "      <td>823.31</td>\n",
              "      <td>2,965,800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1/30/2017</td>\n",
              "      <td>814.66</td>\n",
              "      <td>815.84</td>\n",
              "      <td>799.80</td>\n",
              "      <td>802.32</td>\n",
              "      <td>3,246,600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1/31/2017</td>\n",
              "      <td>796.86</td>\n",
              "      <td>801.25</td>\n",
              "      <td>790.52</td>\n",
              "      <td>796.79</td>\n",
              "      <td>2,160,600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-565c886b-5f28-4bd4-b371-ac4835b6022b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-565c886b-5f28-4bd4-b371-ac4835b6022b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-565c886b-5f28-4bd4-b371-ac4835b6022b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0) "
      ],
      "metadata": {
        "id": "JUCRHCK2SVEQ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_total.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plMok8nPUYls",
        "outputId": "e2648916-7bb4-448f-80cd-47811368a88b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    325.25\n",
              "1    331.27\n",
              "2    329.83\n",
              "3    328.34\n",
              "4    322.04\n",
              "Name: Open, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values  \n"
      ],
      "metadata": {
        "id": "O58mvhPoq4-P"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = inputs.reshape(-1,1)  \n"
      ],
      "metadata": {
        "id": "lhgTvOZKq9EA"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = sc.transform(inputs)  "
      ],
      "metadata": {
        "id": "0JDM1aUVrBkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_test = []  \n",
        "for i in range(60, 80):  \n",
        "    X_test.append(inputs[i-60:i, 0])  \n",
        "X_test = np.array(X_test)  \n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))  \n"
      ],
      "metadata": {
        "id": "pNi7gAHYrHnZ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_stock_price = regressor.predict(X_test)  \n"
      ],
      "metadata": {
        "id": "wB6wDMJBrQSF"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_stock_price = sc.inverse_transform(predicted_stock_price) \n",
        "#dataset_test = pd.read_csv('/content/Google_Stock_Price_Test.csv')  \n",
        "predicted_stock_price"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9xOjuposyxE",
        "outputId": "70a1f640-514d-41b9-a619-11e2b2af99b5"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1608.0519],\n",
              "       [1608.0519],\n",
              "       [1608.0519],\n",
              "       [1608.0519],\n",
              "       [1608.0519],\n",
              "       [1608.0519],\n",
              "       [1608.0519],\n",
              "       [1608.0519],\n",
              "       [1608.0519],\n",
              "       [1608.0519],\n",
              "       [1608.0519],\n",
              "       [1608.0519],\n",
              "       [1608.0519],\n",
              "       [1608.0519],\n",
              "       [1608.0519],\n",
              "       [1608.0521],\n",
              "       [1608.0519],\n",
              "       [1608.052 ],\n",
              "       [1608.052 ],\n",
              "       [1608.0521]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(real_stock_price, color = 'red', label = 'Real Google Stock Price')  \n",
        "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Google Stock Price')  \n",
        "plt.title('Google Stock Price Prediction')  \n",
        "plt.xlabel('Time')  \n",
        "plt.ylabel('Google Stock Price')  \n",
        "plt.legend()  \n",
        "plt.show()  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "E1Q2SOyjukvK",
        "outputId": "d2c0937d-2da9-4e5e-bf92-c8bf397dedc4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}